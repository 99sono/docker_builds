################################
# LOCAL MODELS:
################################

## Specify the model to use for the main chat
# Dense models
# model: ollama_chat/qwen2.5:7b
# model: ollama_chat/gemma3:12b
# model: ollama_chat/gemma3:4b

#  Section containing Mixture of experts model (the less active parameters the more able to run on the CPU)
# mixture of experts model
# model: ollama_chat/qwen3:30b-a3b

################################
# OPEN ROUTER MODELS:
################################
model: openrouter/deepseek/deepseek-r1:free

# Server Configuration property does not exist exist
# We need to export OLLAMA_API_BASE=http://127.0.0.1:11434 
# server: http://localhost:11434
# The Yaml configuration is open-ai-base
## Specify the api base url (uncomment this if using local llms)
#openai-api-base: http://localhost:11434


# If we wan to use models proxied by open router (e.g deep seek r1) 
# to not commit the API key set in ther terminal export OPENAI_API_KEY=WithTheKeyOfOpenRouter
# See: https://aider.chat/docs/llms/openrouter.html
# https://openrouter.ai/api/v1
openai-api-base: https://openrouter.ai/api/v1/chat/completions

###############
# Git Settings:

## Enable/disable looking for a git repo (default: True)
git: true

## Enable/disable adding .aider* to .gitignore (default: True)
gitignore: false

## Specify the aider ignore file (default: .aiderignore in git root)
aiderignore: .aiderignore

## Only consider files in the current subtree of the git repository
#subtree-only: false

## Enable/disable auto commit of LLM changes (default: True)
auto-commits: false

## Run aider in your browser
#gui: false

## Enable/disable suggesting shell commands (default: True)
#suggest-shell-commands: true

set-env:
  - OLLAMA_API_BASE=http://127.0.0.1:11434 

